# Key takeaways

A summary of our overall observations on the tool-set we assessed.

**Fairness**

As is often the case with data-resources and toolkits, there is a gap between **real-life considerations** and the use cases in those tools, often created in an academic vacuum. To make those toolkits truly useful to as many practitioners as possible, care should be taken to **select realistic use cases**, bearing in mind "real life" requirements such as not requiring protected features as input. 

We also observed that many of those tools **assumed a great deal of knowledge** on the part of practitioners regarding the practice of evaluating fairness. 

The methodologies used in the different tools vary greatly - there is little **consistency in approaches or techniques**, and the tools themselves are very different from each other. 

Those tools are intended for use at a very specific point in the data-pipeline, and therefore might be of **limited use in an end-to-end** fairness audit. 

We found a lack of regression implementation vs. academic theory.

**Explanation**

The tools we assessed are often broken and unmaintained. 

**Explainability** is a key component of making algorithms auditable, and should be built-in as part of existing libraries' core. This will help it being taken more seriously and is needed it we want to democratize AI. 

There is a difference between tools vs. usability (mirrors OS and Commercialized products).

**Natural Language Processing (NLP)**

There is a lack of NLP-focused package or library with decent documentation around detecting and removing bias that is model-agnostic. 
The open source tools are not robust or standardised, and lack documentation. 

**Guidelines / checklists**

Ethics should be an **iterative process**, but a lot of the tools are intended for one-off use. How should they be integrated into an extisting pipeline? 
It is not clear what the practitioners are meant to do after those tools have been used, or how to respond to the insights they might find. Clearer **calls to action** would help anchor those tools in realistic scenarios.

**Communicating ethics**

The existing tools are **focused on the US** - some might be of limited use in a global context, where data collection practices and attitudes to data ethics differ.

Are the tools technically accessible? They require some **familiarity with machine learning models** and might be intimidating for complete newcomers. 

The tools we examined were useful, but would need to be complemented with other materials in order to persuade audiences that ethics is a necessary part of development and needs to be embedded throughout a product lifecycle. 
