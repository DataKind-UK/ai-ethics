# Summary

We reviewed two tools that have been designed to communicate ideas about AI ethics: MIT Technology Review’s [Can you make AI fairer than a judge?](https://www.technologyreview.com/2019/10/17/75285/ai-fairer-than-judge-criminal-risk-assessment-algorithm/) tool, that is designed to illustrate tensions between different measures of ‘fairness’, and Google’s [AI Explorables](https://pair.withgoogle.com/explorables/): dynamic illustrations of bias and fairness in machine learning models. Both tools have been designed to illustrate specific use cases of machine learning and ways that bias and unfairness can creep in: while both are accessible to an interested layperson, they both require some familiarity with machine learning, modelling and interpretation, and might be intimidating to complete newcomers.

Both tools also illustrate USA-based examples: in a UK context, these are understandable, but localised examples might help situate ideas of bias and unfairness in a UK context. We found that both tools were useful, but not complete explanations: they could usefully be complemented with broader explanations about why ethics should be considered as intrinsic to development work. Organisations and institutions using AI and machine learning need to understand that ignoring ethical concerns will result in opportunity, reputational or financial costs, not to mention the risk of real harm to individuals: these tools help to illustrate that, but don’t necessarily do the persuasive work required to embed ethics within organisations.
